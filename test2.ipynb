{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d97634-d62a-4ff7-96c8-5fbe1d4290df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T00:28:17.635823Z",
     "iopub.status.busy": "2023-06-14T00:28:17.635355Z",
     "iopub.status.idle": "2023-06-14T02:08:09.194633Z",
     "shell.execute_reply": "2023-06-14T02:08:09.193938Z",
     "shell.execute_reply.started": "2023-06-14T00:28:17.635714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2841\n",
      "Epoch 2/10, Loss: 0.2839\n",
      "Epoch 3/10, Loss: 0.2837\n",
      "Epoch 4/10, Loss: 0.2838\n",
      "Epoch 5/10, Loss: 0.2840\n",
      "Epoch 6/10, Loss: 0.2838\n",
      "Epoch 7/10, Loss: 0.2841\n",
      "Epoch 8/10, Loss: 0.2840\n",
      "Epoch 9/10, Loss: 0.2837\n",
      "Epoch 10/10, Loss: 0.2840\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to load news data and encode categories\n",
    "def load_news_data(filepath):\n",
    "    news = pd.read_csv(filepath, sep='\\t', names=['News_ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'Entities', 'Relations'])\n",
    "    \n",
    "    # Convert categories and subcategories to integers\n",
    "    category_encoder = LabelEncoder()\n",
    "    subcategory_encoder = LabelEncoder()\n",
    "\n",
    "    news['Category'] = category_encoder.fit_transform(news['Category'])\n",
    "    news['SubCategory'] = subcategory_encoder.fit_transform(news['SubCategory'])\n",
    "\n",
    "    return news, len(category_encoder.classes_), len(subcategory_encoder.classes_)\n",
    "\n",
    "# Load entity and relation embeddings\n",
    "entity_embeddings_df = pd.read_csv('./MINDsmall_train/entity_embedding.vec', delimiter='\\t', header=None, index_col=0)\n",
    "entity_embeddings = entity_embeddings_df.values\n",
    "\n",
    "relation_embeddings_df = pd.read_csv('./MINDsmall_train/relation_embedding.vec', delimiter='\\t', header=None, index_col=0)\n",
    "relation_embeddings = relation_embeddings_df.values\n",
    "\n",
    "# Function to pad sequences\n",
    "def pad_sequences(sequences, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = [seq[:max_length] + [0] * max(0, max_length - len(seq)) for seq in sequences]\n",
    "    return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "# Function to get entity/relation embeddings for a sequence\n",
    "def get_embedding_for_sequence(sequence, embedding_dict, embed_dim, device):\n",
    "    embeddings = []\n",
    "    max_length = 0  # 최대 임베딩 길이 초기화\n",
    "    for items in sequence:\n",
    "        item_embeddings = [embedding_dict[item] for item in items if item in embedding_dict]\n",
    "        if item_embeddings:\n",
    "            embeddings.append(torch.sum(torch.stack(item_embeddings), dim=0))\n",
    "            max_length = max(max_length, len(item_embeddings))  # 가장 긴 임베딩 길이 갱신\n",
    "        else:\n",
    "            embeddings.append(torch.zeros(embed_dim, device=device))\n",
    "    \n",
    "    # 모든 임베딩의 길이를 가장 긴 임베딩 길이로 맞춤\n",
    "    embeddings = [F.pad(embeddings[i], (0, max_length - len(item_embeddings)), 'constant', 0).unsqueeze(0) for i, item_embeddings in enumerate(embeddings)]\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# Read behaviors.tsv and news.tsv files\n",
    "behaviors = pd.read_csv('./MINDsmall_train/behaviors.tsv', delimiter='\\t', header=None)\n",
    "news, num_categories, num_subcategories = load_news_data('./MINDsmall_train/news.tsv')\n",
    "news_ids = news['News_ID'].unique()\n",
    "\n",
    "# Assign column names\n",
    "behaviors.columns = ['Impression ID', 'User ID', 'Time', 'History', 'Impressions']\n",
    "\n",
    "# Assign integer indices to user and news IDs\n",
    "user_ids = behaviors['User ID'].unique()\n",
    "user2idx = {o: i for i, o in enumerate(user_ids)}\n",
    "idx2user = {i: o for i, o in enumerate(user_ids)}\n",
    "\n",
    "news2idx = {o: i for i, o in enumerate(news_ids)}\n",
    "idx2news = {i: o for i, o in enumerate(news_ids)}\n",
    "\n",
    "# Replace user and news IDs with integer indices\n",
    "behaviors['User ID'] = behaviors['User ID'].apply(lambda x: user2idx[x])\n",
    "behaviors['History'] = behaviors['History'].fillna('')\n",
    "behaviors['History'] = behaviors['History'].apply(lambda x: [news2idx[i] for i in x.split() if i in news2idx])\n",
    "\n",
    "# Replace impressions with integer indices\n",
    "news2idx['unknown'] = len(news2idx)\n",
    "\n",
    "def process_impressions(impressions):\n",
    "    return [news2idx.get(i.split('-')[0], news2idx['unknown']) for i in impressions.split()]\n",
    "\n",
    "behaviors['Impressions'] = behaviors['Impressions'].apply(process_impressions)\n",
    "# Split train and test behaviors\n",
    "train_behaviors, test_behaviors = train_test_split(behaviors, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, user_tensor, sequence_tensor, impression_tensor, category_tensor, subcategory_tensor, entity_tensor, relation_tensor):\n",
    "        self.user_tensor = user_tensor\n",
    "        self.sequence_tensor = sequence_tensor\n",
    "        self.impression_tensor = impression_tensor\n",
    "        self.category_tensor = category_tensor\n",
    "        self.subcategory_tensor = subcategory_tensor\n",
    "        self.entity_tensor = entity_tensor\n",
    "        self.relation_tensor = relation_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.user_tensor[index],\n",
    "            self.sequence_tensor[index],\n",
    "            self.impression_tensor[index % self.impression_tensor.shape[0]],\n",
    "            self.category_tensor[index % self.category_tensor.shape[0]],\n",
    "            self.subcategory_tensor[index % self.subcategory_tensor.shape[0]],\n",
    "            self.entity_tensor[index % self.entity_tensor.shape[0]],\n",
    "            self.relation_tensor[index % self.relation_tensor.shape[0]]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sequence_tensor.shape[0]\n",
    "\n",
    "# Define the model\n",
    "class CausalRec(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_categories, num_subcategories, embed_dim, num_gru_layers, device):\n",
    "        super(CausalRec, self).__init__()\n",
    "        \n",
    "        self.num_items = num_items\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_dim)\n",
    "        self.category_embedding = nn.Embedding(num_categories, embed_dim)\n",
    "        self.subcategory_embedding = nn.Embedding(num_subcategories, embed_dim)\n",
    "\n",
    "        self.gru = nn.GRU(embed_dim, embed_dim, num_layers=num_gru_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "        self.lambda_ = torch.zeros(1, requires_grad=False).to(device)\n",
    "        self.mu_ = torch.zeros(1, requires_grad=False).to(device)\n",
    "\n",
    "        self.temperature = 1.0\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def forward(self, user_indices, sequence, item_indices, category_indices, subcategory_indices, entities, relations):\n",
    "        user_embed = self.user_embedding(user_indices)\n",
    "        item_embed = self.item_embedding(item_indices % self.num_items)\n",
    "\n",
    "        category_embed = self.category_embedding(category_indices).unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "        subcategory_embed = self.subcategory_embedding(subcategory_indices).unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "        entity_embed = get_embedding_for_sequence(entities, entity_embeddings, embed_dim, device).unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "        relation_embed = get_embedding_for_sequence(relations, relation_embeddings, embed_dim, device).unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "\n",
    "        item_embed = item_embed.unsqueeze(2).expand(-1, -1, category_embed.size(1), -1)\n",
    "        item_embed = item_embed[:, :1, :4, :]  # 크기를 (4, 1, 4, 100)로 변경\n",
    "\n",
    "\n",
    "        out, _ = self.gru(sequence)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        user_embed_expanded = user_embed.unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "        out_expanded = out.unsqueeze(1).expand(-1, item_embed.size(1), -1)\n",
    "\n",
    "        prob = torch.sigmoid((user_embed_expanded * item_embed).sum(dim=-1, keepdim=True) + out_expanded)\n",
    "        \n",
    "        # constraint라는 제약조건을 설정, prob의 평균(클릭확률 vs Non클릭확률)이 0.5에 가깝게 유지\n",
    "        constraint = (prob.mean() - 0.5) ** 2\n",
    "        self.lambda_ = self.lambda_ + self.mu_ * constraint\n",
    "        \n",
    "        # Gumbel softmax - Reparametrization\n",
    "        u = torch.rand(prob.shape).to(device)\n",
    "        gumbel_noise = -torch.log(-torch.log(u + self.eps) + self.eps)\n",
    "        logit = (prob + gumbel_noise) / self.temperature\n",
    "        prob = torch.sigmoid(logit)\n",
    "\n",
    "        return prob\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "num_users = len(user_ids)\n",
    "num_items = len(news_ids)\n",
    "embed_dim = 100\n",
    "num_gru_layers = 2\n",
    "max_length = 100\n",
    "\n",
    "# Instantiate the model\n",
    "model = CausalRec(num_users, num_items, num_categories, num_subcategories, embed_dim, num_gru_layers, device)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = NewsDataset(\n",
    "    torch.tensor(train_behaviors['User ID'].values, dtype=torch.long),\n",
    "    pad_sequences(train_behaviors['History'].values, max_length).float(),\n",
    "    pad_sequences(train_behaviors['Impressions'].values).float(),\n",
    "    torch.tensor(news['Category'].values, dtype=torch.long),\n",
    "    torch.tensor(news['SubCategory'].values, dtype=torch.long),\n",
    "    entity_embeddings,\n",
    "    relation_embeddings\n",
    ")\n",
    "test_dataset = NewsDataset(\n",
    "    torch.tensor(test_behaviors['User ID'].values, dtype=torch.long),\n",
    "    pad_sequences(test_behaviors['History'].values, max_length).float(),\n",
    "    pad_sequences(test_behaviors['Impressions'].values).float(),\n",
    "    torch.tensor(news['Category'].values, dtype=torch.long),\n",
    "    torch.tensor(news['SubCategory'].values, dtype=torch.long),\n",
    "    entity_embeddings,\n",
    "    relation_embeddings\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "eta = 1.1\n",
    "delta = 0.1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for user_indices, sequence, item_indices, category_indices, subcategory_indices, entities, relations in train_loader:\n",
    "        user_indices = user_indices.to(device)\n",
    "        sequence = sequence.to(device)\n",
    "        item_indices = item_indices.to(device, dtype=torch.long)\n",
    "        category_indices = category_indices.to(device)\n",
    "        subcategory_indices = subcategory_indices.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            user_indices, sequence, item_indices, category_indices, subcategory_indices, entities, relations\n",
    "        )\n",
    "\n",
    "        labels = torch.ones_like(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 제약조건이 만족되지 않을 경우(constraint가 delta * prev_constraint보다 큰 경우), Lagrange multiplier인 mu_를 eta배 증가시킴으로써 제약조건을 강화 \n",
    "        constraint = (outputs.mean() - 0.5) ** 2\n",
    "        model.lambda_ = model.lambda_ + model.mu_ * constraint\n",
    "\n",
    "        if constraint > delta * prev_constraint:\n",
    "            model.mu_ = eta * model.mu_\n",
    "\n",
    "        prev_constraint = constraint\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f09dbab-bf31-4538-acfe-3580f0cc36aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T05:58:58.388434Z",
     "iopub.status.busy": "2023-06-14T05:58:58.387844Z",
     "iopub.status.idle": "2023-06-14T06:00:47.037840Z",
     "shell.execute_reply": "2023-06-14T06:00:47.036712Z",
     "shell.execute_reply.started": "2023-06-14T05:58:58.388374Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@1:  0.030449738820231876\n",
      "Hit@5:  0.14995540833227164\n",
      "NDCG@5:  0.0006621525547102801\n"
     ]
    }
   ],
   "source": [
    "def hit_at_k(predictions, targets, k):\n",
    "    '''\n",
    "    Calculate hit@k score\n",
    "    predictions: sorted list of predictions\n",
    "    targets: list of targets\n",
    "    '''\n",
    "    top_k_preds = predictions[:k]\n",
    "    if len(set(top_k_preds).intersection(set(targets))) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def ndcg_at_k(predictions, targets, k):\n",
    "    '''\n",
    "    Calculate ndcg@k score\n",
    "    predictions: sorted list of predictions\n",
    "    targets: list of targets\n",
    "    '''\n",
    "    dcg = 0\n",
    "    for i, pred in enumerate(predictions[:k]):\n",
    "        if pred in targets:\n",
    "            dcg += 1 / np.log2(i + 2)  # i + 2 because i starts at 0 and the formula starts at 1\n",
    "\n",
    "    idcg = 0  # ideal dcg; best possible dcg given the targets\n",
    "    for i in range(len(targets)):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "        \n",
    "    return dcg / idcg\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "\n",
    "hit1_total = 0\n",
    "hit5_total = 0\n",
    "ndcg5_total = 0\n",
    "num_batches = 0\n",
    "\n",
    "for user_indices, sequence, item_indices, category_indices, subcategory_indices, entities, relations in test_loader:\n",
    "    user_indices = user_indices.to(device)\n",
    "    sequence = sequence.to(device)\n",
    "    item_indices = item_indices.to(device, dtype=torch.long)\n",
    "    category_indices = category_indices.to(device)\n",
    "    subcategory_indices = subcategory_indices.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            user_indices, sequence, item_indices, category_indices, subcategory_indices, entities, relations\n",
    "        )\n",
    "\n",
    "    predictions = outputs.flatten().argsort().tolist()\n",
    "    targets = item_indices.flatten().tolist()\n",
    "\n",
    "    hit1_total += hit_at_k(predictions, targets, 1)\n",
    "    hit5_total += hit_at_k(predictions, targets, 5)\n",
    "    ndcg5_total += ndcg_at_k(predictions, targets, 5)\n",
    "\n",
    "    num_batches += 1\n",
    "\n",
    "hit1 = hit1_total / num_batches\n",
    "hit5 = hit5_total / num_batches\n",
    "ndcg5 = ndcg5_total / num_batches\n",
    "\n",
    "print(\"Hit@1: \", hit1)\n",
    "print(\"Hit@5: \", hit5)\n",
    "print(\"NDCG@5: \", ndcg5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c24ffd-e802-42ed-8c7a-6b73a69fd9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
